{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Automation for OpenShift on IBM Power Systems Contents: OCS-UPI-KVM Overview User Requirements Scripts Chron tab Automation OCP4-UPI-PowerVS Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install OCP4-UPI-KVM Introduction Prerequisites Image and VM Requirements OCP Install OCP4-UPI-PowerVM Introduction Prerequisites Image and LPAR Requirements OCP Install PowerVS_Automation OCP4-Playbooks Introduction Assumptions Bastion and HA Setup Terraform-Provider-IBM Terraform Provider Requirements IBM Clouds Ansible Modules","title":"Home"},{"location":"#welcome-to-automation-for-openshift-on-ibm-power-systems","text":"","title":"Welcome to Automation for OpenShift on IBM Power Systems"},{"location":"#contents","text":"OCS-UPI-KVM Overview User Requirements Scripts Chron tab Automation OCP4-UPI-PowerVS Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install OCP4-UPI-KVM Introduction Prerequisites Image and VM Requirements OCP Install OCP4-UPI-PowerVM Introduction Prerequisites Image and LPAR Requirements OCP Install PowerVS_Automation OCP4-Playbooks Introduction Assumptions Bastion and HA Setup Terraform-Provider-IBM Terraform Provider Requirements IBM Clouds Ansible Modules","title":"Contents:"},{"location":"automation/","text":"OpenShift 4 on PowerVS Automation This repo contain a bash script which can help you deploy OpenShift Container Platform 4.X on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . The script make use of Terraform configurations from ocp4-upi-powervs . Do check out the PowerVS Automation . What can the script do It can run on multiple x86 platforms including where terraform runs out of the box: Mac OSX (Darwin) Linux (x86_64) Windows 10 (Git Bash & Cygwin) It can setup the latest IBM Cloud CLI and Terraform for you. It can help you populate variables for ocp4-upi-powervs via interactive prompts. It can help you create an OpenShift cluster for you on PowerVS. Thanks to the project ocp4-upi-powervs . How to use Just create an install directory and download the script on the box. curl https://raw.githubusercontent.com/ocp-power-automation/powervs_automation/master/deploy.sh -o deploy.sh && chmod +x deploy.sh You are good to run the script: # ./deploy.sh Automation for deploying OpenShift 4.X on PowerVS Usage: ./deploy.sh [command] [<args> [<value>]] Available commands: setup Install all required packages/binaries in current directory variables Interactive way to populate the variables file create Create an OpenShift cluster destroy Destroy an OpenShift cluster output Display the cluster information. Runs terraform output [NAME] help Display this information Where <args>: -trace Enable tracing of all executed commands -verbose Enable verbose for terraform console -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) Submit issues at: https://github.com/ocp-power-automation/ocp4-upi-powervs/issues","title":"PowerVS_Automation"},{"location":"automation/#openshift-4-on-powervs-automation","text":"This repo contain a bash script which can help you deploy OpenShift Container Platform 4.X on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . The script make use of Terraform configurations from ocp4-upi-powervs . Do check out the PowerVS Automation .","title":"OpenShift 4 on PowerVS Automation"},{"location":"automation/#what-can-the-script-do","text":"It can run on multiple x86 platforms including where terraform runs out of the box: Mac OSX (Darwin) Linux (x86_64) Windows 10 (Git Bash & Cygwin) It can setup the latest IBM Cloud CLI and Terraform for you. It can help you populate variables for ocp4-upi-powervs via interactive prompts. It can help you create an OpenShift cluster for you on PowerVS. Thanks to the project ocp4-upi-powervs .","title":"What can the script do"},{"location":"automation/#how-to-use","text":"Just create an install directory and download the script on the box. curl https://raw.githubusercontent.com/ocp-power-automation/powervs_automation/master/deploy.sh -o deploy.sh && chmod +x deploy.sh You are good to run the script: # ./deploy.sh Automation for deploying OpenShift 4.X on PowerVS Usage: ./deploy.sh [command] [<args> [<value>]] Available commands: setup Install all required packages/binaries in current directory variables Interactive way to populate the variables file create Create an OpenShift cluster destroy Destroy an OpenShift cluster output Display the cluster information. Runs terraform output [NAME] help Display this information Where <args>: -trace Enable tracing of all executed commands -verbose Enable verbose for terraform console -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) Submit issues at: https://github.com/ocp-power-automation/ocp4-upi-powervs/issues","title":"How to use"},{"location":"introduction/","text":"Introduction Red Hat and IBM Power teams have been working together on OpenShift since 2018 with the initial release of OpenShift 3.11 on Power. Through this continuous collaboration and effort between IBM and Red Hat, improvements on Power have been made such as the support of IBM PowerVM with OpenShift to enable our Power Enterprise systems. OpenShift on IBM Power Systems takes advantage of the Hybrid Cloud Flexibility, as a result of it, OpenShift can be deployed on PowerVM or Red Hat KVM (development) on Power scale-out servers to exploit the 3.2x container density advantage per core of the POWER9 multi-threaded architecture. Automation for OpenShift on IBM Power Architecture can make things easier and much faster for the end user, making the entire process hasslefree allowing Power clients to exploit the innovative new capabilities.A deployment host is any virtual or physical host that is typically required for the installation of Red Hat OpenShift. The Red Hat OpenShift installation assumes that many, if not all the external services like DNS, load balancing, HTTP server, DHCP are already available in an existing data center and therefore there is no need to duplicate them on a node in the Red Hat OpenShift cluster. Master node, worker node and bootstrap can run on top of PowerVC, PowerVM, Red Hat Virtualization, KVM or run bare metal environments. You can manage trust configuration directly on each node or manage the files on a separate host by distributing them to the appropriate nodes using Ansible. Ansible is an automation tool used to configure systems, deploy software, and perform rolling updates. Ansible includes support for container-native virtualization, and Ansible modules enable you to automate cluster management tasks such as template, persistent volume claim, and virtual machine operations. Ansible provides a way to automate container-native virtualization management, which you can also accomplish by using the oc CLI tool or APIs. Ansible is unique because it allows you to integrate KubeVirt modules with other Ansible modules. Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material. At a basic level, playbooks can be used to manage configurations of and deployments to remote machines. At a more advanced level, they can sequence multi-tier rollouts involving rolling updates, and can delegate actions to other hosts, interacting with monitoring servers and load balancers along the way. Playbooks are designed to be human-readable and are developed in a basic text language. There are multiple ways to organize playbooks and the files they include. The Playbooks are used for installation of OCP on Power and other post install customizations Terraform is an Open Source software that is developed by HashiCorp that enables predictable and consistent provisioning of IBM Cloud platform, classic infrastructure, and VPC infrastructure resources by using a high-level scripting language. Terraform is used to create, manage, and update infrastructure resources such as physical machines, VMs, network switches, containers, and more. Terraform can be used to automate IBM Cloud resource provisioning, rapidly build complex, multi-tier cloud environments, and enable Infrastructure as Code (IaC). A lot of OpenShift 4 specific jargon is used throughout this doc, so please visit the official documentation page to get familiar with OpenShift 4.","title":"Introduction"},{"location":"introduction/#introduction","text":"Red Hat and IBM Power teams have been working together on OpenShift since 2018 with the initial release of OpenShift 3.11 on Power. Through this continuous collaboration and effort between IBM and Red Hat, improvements on Power have been made such as the support of IBM PowerVM with OpenShift to enable our Power Enterprise systems. OpenShift on IBM Power Systems takes advantage of the Hybrid Cloud Flexibility, as a result of it, OpenShift can be deployed on PowerVM or Red Hat KVM (development) on Power scale-out servers to exploit the 3.2x container density advantage per core of the POWER9 multi-threaded architecture. Automation for OpenShift on IBM Power Architecture can make things easier and much faster for the end user, making the entire process hasslefree allowing Power clients to exploit the innovative new capabilities.A deployment host is any virtual or physical host that is typically required for the installation of Red Hat OpenShift. The Red Hat OpenShift installation assumes that many, if not all the external services like DNS, load balancing, HTTP server, DHCP are already available in an existing data center and therefore there is no need to duplicate them on a node in the Red Hat OpenShift cluster. Master node, worker node and bootstrap can run on top of PowerVC, PowerVM, Red Hat Virtualization, KVM or run bare metal environments. You can manage trust configuration directly on each node or manage the files on a separate host by distributing them to the appropriate nodes using Ansible. Ansible is an automation tool used to configure systems, deploy software, and perform rolling updates. Ansible includes support for container-native virtualization, and Ansible modules enable you to automate cluster management tasks such as template, persistent volume claim, and virtual machine operations. Ansible provides a way to automate container-native virtualization management, which you can also accomplish by using the oc CLI tool or APIs. Ansible is unique because it allows you to integrate KubeVirt modules with other Ansible modules. Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material. At a basic level, playbooks can be used to manage configurations of and deployments to remote machines. At a more advanced level, they can sequence multi-tier rollouts involving rolling updates, and can delegate actions to other hosts, interacting with monitoring servers and load balancers along the way. Playbooks are designed to be human-readable and are developed in a basic text language. There are multiple ways to organize playbooks and the files they include. The Playbooks are used for installation of OCP on Power and other post install customizations Terraform is an Open Source software that is developed by HashiCorp that enables predictable and consistent provisioning of IBM Cloud platform, classic infrastructure, and VPC infrastructure resources by using a high-level scripting language. Terraform is used to create, manage, and update infrastructure resources such as physical machines, VMs, network switches, containers, and more. Terraform can be used to automate IBM Cloud resource provisioning, rapidly build complex, multi-tier cloud environments, and enable Infrastructure as Code (IaC). A lot of OpenShift 4 specific jargon is used throughout this doc, so please visit the official documentation page to get familiar with OpenShift 4.","title":"Introduction"},{"location":"ocp_kvm/","text":"Table of Contents Introduction Pre-requisites Image and VM requirements OCP Install Contributing Introduction This repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using libvirt. This project leverages the following ansible playbook to setup helper node (bastion) for OCP deployment. Run this code from either Mac or Linux (Intel) system. For automating setup, see OCP-UPI-KVM :heavy_exclamation_mark: This automation is intended for test/development purposes only and there is no formal support. For issues please open a GitHub issue Pre-requisites Git : Please refer to the following link for instructions on installing git for Linux and Mac. Terraform >= 0.13 : Please refer to the following link for instructions on installing terraform for Linux and Mac. For validating the version run terraform version command after install. Terraform Providers : Please ensure terraform providers are built and installed on Terraform Client Machine. You can follow the Build Terraform Providers guide. libvirt : Please ensure libvirt is installed and configured on the KVM host. You can follow the Libvirt Host setup guide. Image and VM requirements For information on how to configure the images required for the automation see Preparing Images for Power . Following are the recommended VM configs for OpenShift nodes that will be deployed with RHCOS image. - Bootstrap, Master - 4 vCPUs, 16GB RAM, 120 GB Disk This config is suitable for majority of the scenarios - Worker - 4 vCPUs, 16GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Following is the recommended VM config for the helper node that will be deployed with RHEL 8.0 (or later) image. - Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk OCP Install Follow these quickstart steps to kickstart OCP installation on Power KVM using libvirt. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"OCP4-UPI-KVM"},{"location":"ocp_kvm/#table-of-contents","text":"Introduction Pre-requisites Image and VM requirements OCP Install Contributing","title":"Table of Contents"},{"location":"ocp_kvm/#introduction","text":"This repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using libvirt. This project leverages the following ansible playbook to setup helper node (bastion) for OCP deployment. Run this code from either Mac or Linux (Intel) system. For automating setup, see OCP-UPI-KVM :heavy_exclamation_mark: This automation is intended for test/development purposes only and there is no formal support. For issues please open a GitHub issue","title":"Introduction"},{"location":"ocp_kvm/#pre-requisites","text":"Git : Please refer to the following link for instructions on installing git for Linux and Mac. Terraform >= 0.13 : Please refer to the following link for instructions on installing terraform for Linux and Mac. For validating the version run terraform version command after install. Terraform Providers : Please ensure terraform providers are built and installed on Terraform Client Machine. You can follow the Build Terraform Providers guide. libvirt : Please ensure libvirt is installed and configured on the KVM host. You can follow the Libvirt Host setup guide.","title":"Pre-requisites"},{"location":"ocp_kvm/#image-and-vm-requirements","text":"For information on how to configure the images required for the automation see Preparing Images for Power . Following are the recommended VM configs for OpenShift nodes that will be deployed with RHCOS image. - Bootstrap, Master - 4 vCPUs, 16GB RAM, 120 GB Disk This config is suitable for majority of the scenarios - Worker - 4 vCPUs, 16GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Following is the recommended VM config for the helper node that will be deployed with RHEL 8.0 (or later) image. - Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk","title":"Image and VM requirements"},{"location":"ocp_kvm/#ocp-install","text":"Follow these quickstart steps to kickstart OCP installation on Power KVM using libvirt.","title":"OCP Install"},{"location":"ocp_kvm/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocs_kvm/","text":"Overview Provide scripts enabling the automation of OCS-CI on IBM Power Servers, including the ability to create an OCP cluster, run OCS-CI tests, and destroy OCS (and OCP). OCS-CI provides a deployment option to install OpenShift Container Storage on the given worker nodes based on Red Hat Ceph Storage. Parameters related to the definition of the cluster such as the OpenShift Version and the number and size of worker nodes are specified via environment variables to the scripts listed below. The goal of this project is to provide the primitives that are needed to enable OCS-CI on Power Servers. These scripts form a framework for the development and validation of OpenShift clusters and operators. This project utilizes KVM to create a OpenShift Cluster running in VMs. This project runs on baremetal servers as well as PowerVM and PowerVS based servers provided that a large enough LPAR is allocated. For automating setup, see OCS-UPI-KVM User Requirements This project may be run by non-root users with sudo authority. passwordless sudo access should be enabled, so that the scripts don't prompt for a password. A helper script is provided for this purpose at scripts/helper/set-passwordless-sudo.sh . There are no command arguments for this script and it should be run once during initial setup. Note: Non-root users must use the sudo command with virsh to see VMs Scripts The scripts below correspond to high level tasks of OCS-CI. They are intended to be invoked from an automation test framework such as Jenkins . The scripts are listed in the order that they are expected to be run. create-ocp.sh [ --retry ] setup-ocs-ci.sh deploy-ocs-ci.sh test-ocs-ci.sh [ --tier <0,1,...> ] teardown-ocs-ci.sh destroy-ocp.sh This project uses the following git submodules: github.com/ocp-power-automation/ocp4-upi-kvm github.com/red-hat-storage/ocs-ci These underlying projects must be instantiated before the create, setup, deploy, test, and teardown scripts are used. The user is expected to setup the submodules before invoking these scripts. The workflow sample scripts described in the next section provide some end to end work flows which of necessity instantiate submodules. These sample scripts may be copied to the workspace directory and edited as desired to customize a work flow. Most users are expected to do this. The information provided below describes some of the dynamics surrounding the create, deploy, and test scripts. First, there are two ways to instantiate submodules as shown below: git clone https://github.com/ocp-power-automation/ocs-upi-kvm --recursive cd ocs-upi-kvm/scripts OR git clone https://github.com/ocp-power-automation/ocs-upi-kvm.sh cd ocs-upi-kvm git submodule update --init The majority of the create-ocp.sh command is spent running terraform (and ansible). On occasion, a transient error will occur while creating the cluster. In this case, the operation can be restarted by specifying the --retry argument. This can save half an hour of execution time. If this argument is not specified, the existing cluster will be torn down automatically assuming there is one. If a failure occurs while running the deploy-ocs-ci.sh script, the operation has to be restarted from the beginning. That is to say with creat-ocp.sh . Do not specify the --retry argument as the OCP cluster has to be completely removed before trying to deploy OCS. The ocs-ci project alters the state of the OCP cluster. Further, the teardown-ocs-ci.sh script has never been obcserved to work cleanly. This simply invokes the underlying ocs-ci function. It is provided as it may be fixed in time and it is a valuable function, if only in theory now. The script destroy-ocp.sh which recompletely removes ocp and ocs. The script create-ocp.sh will also remove an existing OCP cluster if one is present before creating a new one as only one OCP cluster is supported on the host KVM server at a time . This is true even if the cluster was created by another user, so if you are concerned with impacting other users run this command first, sudo virsh list --all . Workflow Sample Scripts samples/dev-ocs.sh [--retry-ocp] [--latest-ocs] samples/test-ocs.sh [--retry-ocp] [--latest-ocs] These scripts are useful in getting started. They implement the full sequence of high level tasks defined above. The test-ocs.sh invokes ocs-ci tier tests 2, 3, 4, 4a, 4b, and 4c. Both scripts designate the use of file backed Ceph disks which are based on qcow2 files. These files are sparsely populated enabling the use of servers with as little as 256 GBs of storage, depending on the number of worker nodes that are requested. The use of file backed data disks is the default. The test-ocs scripts include comments showing how physical disk partitions may be used instead which may improve performance and resilience. As noted above, these scripts may be relocated, customized, and invoked from the workspace directory. Required Environment Variables RHID_USERNAME=xxx RHID_PASSWORD=yyy OR RHID_ORG=ppp RHID_KEY=qqq Project Workspace This project is designed to be used in an automated test framework like Jenkins which utilizes dedicated workspaces to run jobs in parallel on the same server. As such, all input, output, and internally generated files are restricted to the specific workspace instance that is allocated to run the job. For this project, that workspace is assumed to be the parent directory of the cloned project ocs-upi-kvm itself. Required Files These files should be placed in the workspace directory. ~/auth.yaml ~/pull-secret.txt ~/$BASTION_IMAGE The auth.yaml file is required for the script deploy-ocs-ci.sh. It contains secrets for quay and quay.io/rhceph-dev which are obtained from the Redhat OCS-CI team. The pull-secret.txt is required for the scripts create-ocp.sh and deploy-ocs-ci.sh. Download your managed pull secrets from https://cloud.redhat.com/openshift/install/pull-secret and add the secret for quay.io/rhceph-dev noted above to the pull-secret.txt file. You will also need to add the secret for registry.svc.ci.openshift.org which may be obtained as follows: Become a member of openshift organization login to https://api.ci.openshift.org/console/catalog Click on \"copy login command\" under username in the right corner. (This will copy the oc login command to your clipboard.) Now open your terminal, paste from the clipboard buffer and execute that command you just pasted (oc login). Execute the oc registry login --registry registry.svc.ci.openshift.org which will store your token in ~/.docker/config.json. The bastion image is a prepared image downloaded from the Red Hat Customer Portal following these instructions . It is named by the environment variable BASTION_IMAGE which has a default value of \"rhel-8.2-update-2-ppc64le-kvm.qcow2\". This is the name of the file downloaded from the RedHat website. When preparing the bastion image above, the root password must be set to 123456 . Optional Environment Variables with Default Values OCP_VERSION=${OCP_VERSION:=\"4.5\"} CLUSTER_DOMAIN=${CLUSTER_DOMAIN:=\"tt.testing\"} BASTION_IMAGE=${BASTION_IMAGE:=\"rhel-8.2-update-2-ppc64le-kvm.qcow2\"} MASTER_DESIRED_CPU=${MASTER_DESIRED_CPU:=\"4\"} MASTER_DESIRED_MEM=${MASTER_DESIRED_MEM:=\"16384\"} WORKER_DESIRED_CPU=${WORKER_DESIRED_CPU:=\"16\"} WORKER_DESIRED_MEM=${WORKER_DESIRED_MEM:=\"65536\"} WORKERS=${WORKERS:=3} IMAGES_PATH=${IMAGES_PATH:=\"/var/lib/libvirt/images\"} OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=${OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE:=\"\"} DATA_DISK_SIZE=${DATA_DISK_SIZE:=256} DATA_DISK_LIST=${DATA_DISK_LIST:=\"\"} FORCE_DISK_PARTITION_WIPE=${FORCE_DISK_PARTITION_WIPE:=\"false\"} CHRONY_CONFIG=${CHRONY_CONFIG:=\"true\"} RHCOS_RELEASE=${RHCOS_RELEASE:=\"\"} Disk sizes are in GBs. The CHRONY_CONFIG parameter above enables NTP servers as OCS CI expects them to be configured. If that is not applicable, then this parameter should probably be set to false. The RHCOS_RELEASE parameter is specific to the OCP_VERSION and is set internally to the latest available rhcos image available provided that it is not set by the user. The internal setting may be outdated. The supported OCP_VERSION s are 4.4 - 4.7. Set a new value like this: export OCP_VERSION=4.6 export RHCOS_RELEASE=4.6.1 The OpenShift installer uses the latest available image which by default is a development build. For released OCP versions, this tool will chose a recently released image based on the environment OCP_VERSION. This image may not be latest available version. This internal selection may be overriden by setting the environment variable OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE to a development preview image . Set a specific daily build like this: REGISTRY=\"registry.svc.ci.openshift.org/ocp-ppc64le/release-ppc64le\" export OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=\"$REGISTRY:4.6.0-0.nightly-ppc64le-2020-09-27-075246\" The script create-ocp.sh will add a data disk to each worker node. This disk is presented inside the worker node as /dev/vdc. In the KVM host server OS, the data disk is backed by either a file or a physical disk partition. If you specify the environment variable DATA_DISK_LIST, then the named physical disk partitions (/dev) will be used. The list is composed of comma separated unique partition names with one partition name specified per worker node. For example, export DATA_DISK_LIST=\"sdi1,sdi2,sdi3\" Otherwise, the data disks will be backed by a file. The environment variable DATA_DISK_SIZE controls the size of the file allocation. If you don't want the extra disk to be allocated, then set DATA_DISK_SIZE=0. In this case, don't run the scripts setup-ocs-ci.sh or deploy-ocs-ci.sh as they will fail. The environment variable FORCE_DISK_PARTITION_WIPE may be set to 'true' to wipe the data on a hard disk partition assuming the environment variable DATA_DISK_LIST is specified. The wipe may take an hour or more to complete. Post Creation of The OpenShift Cluster Build artifacts are placed in the workspace directory which is defined as the parent directory of this github project ocs-upi-kvm . The examples shown below use a dedicated directory for this purpose as there are quite a few output files, some of which are not shown below such as rpms and tar files that are downloaded during cluster creation. The oc Command Upon successful completion of the create-ocp.sh script, the oc command may be invoked in the following way: [user@kvm-host workspace]; source env-ocp.sh [user@kvm-host workspace]; oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 40h v1.19.0+d59ce34 master-1 Ready master 40h v1.19.0+d59ce34 master-2 Ready master 40h v1.19.0+d59ce34 worker-0 Ready worker 39h v1.19.0+d59ce34 worker-1 Ready worker 39h v1.19.0+d59ce34 worker-2 Ready worker 39h v1.19.0+d59ce34 The env-ocp.sh script exports KUBECONFIG and updates the PATH environment variable. It may be useful in some cases to stick these in your user profile. Log Files The following log files are produced: [user@kvm-host-ahv workspace]$ ls -lt *log -rw-rw-r--. 1 luke luke 409491 Oct 23 18:36 create-ocp.log -rw-rw-r--. 1 luke luke 654998 Oct 23 19:06 deploy-ocs-ci.log -rw-rw-r--. 1 luke luke 1144731 Oct 22 23:23 ocs-ci.git.log -rw-rw-r--. 1 luke luke 18468 Oct 23 18:38 setup-ocs-ci.log -rw-rw-r--. 1 luke luke 23431620 Oct 23 18:35 terraform.log -rw-rw-r--. 1 luke luke 29235845 Oct 25 00:30 test-ocs-ci.log Remote Webconsole Support The cluster create command outputs webconsole information which should look something like the first entry below. This information needs to be added to your /etc/hosts file or MacOS equivalent defining the IP Address of the KVM host server. You must generate the companion oauth definition as shown below following the same pattern. <ip kvm host server> console-openshift-console.apps.test-ocp4-5.tt.testing oauth-openshift.apps.test-ocp4-5.tt.testing The browser should prompt you to login to the OCP cluster. The user name is kubeadmin and the password is located in the file /auth/kubeadmin-password . Chrontab Automation The following two files have been provided: chron-ocs.sh test-chron-ocs.sh The chron-ocs.sh script is the master chrontab commandline script. It is located in the scripts/helper directory. The test-chron-ocs.sh script is invoked by chron-ocs.sh and provides the end-to-end OCP/OCS command flow. Presently, this script invokes tier tests 2, 3, 4, 4a, 4b and 4c. You can limit the tests to a subset by editing this file. This file is located in the samples directory. To setup chrontab automation, you must: Create test user account with sudo authority and login to it git clone this project in $HOME and invoke scripts/helper/set-passwordless-sudo.sh Place the required files defined by the ocs-upi-kvm project in $HOME Copy the two chron scripts listed above to $HOME Edit the four lines below in test-chron-ocs.sh : export RHID_USERNAME=<Your RedHat Subscription id> export RHID_PASSWORD=<RedHat Subscription password> export OCP_VERSION=4.5 export IMAGES_PATH=/home/libvirt/images Invoke crontab -e and enter the following two lines: SHELL=/bin/bash 0 0 * * * ~/chron-ocs.sh The example above will invoke chron-ocs.sh every 24 hours at midnight local time. Log files are written to the logs directory under the user's home directory.","title":"OCS-UPI-KVM"},{"location":"ocs_kvm/#overview","text":"Provide scripts enabling the automation of OCS-CI on IBM Power Servers, including the ability to create an OCP cluster, run OCS-CI tests, and destroy OCS (and OCP). OCS-CI provides a deployment option to install OpenShift Container Storage on the given worker nodes based on Red Hat Ceph Storage. Parameters related to the definition of the cluster such as the OpenShift Version and the number and size of worker nodes are specified via environment variables to the scripts listed below. The goal of this project is to provide the primitives that are needed to enable OCS-CI on Power Servers. These scripts form a framework for the development and validation of OpenShift clusters and operators. This project utilizes KVM to create a OpenShift Cluster running in VMs. This project runs on baremetal servers as well as PowerVM and PowerVS based servers provided that a large enough LPAR is allocated. For automating setup, see OCS-UPI-KVM","title":"Overview"},{"location":"ocs_kvm/#user-requirements","text":"This project may be run by non-root users with sudo authority. passwordless sudo access should be enabled, so that the scripts don't prompt for a password. A helper script is provided for this purpose at scripts/helper/set-passwordless-sudo.sh . There are no command arguments for this script and it should be run once during initial setup. Note: Non-root users must use the sudo command with virsh to see VMs","title":"User Requirements"},{"location":"ocs_kvm/#scripts","text":"The scripts below correspond to high level tasks of OCS-CI. They are intended to be invoked from an automation test framework such as Jenkins . The scripts are listed in the order that they are expected to be run. create-ocp.sh [ --retry ] setup-ocs-ci.sh deploy-ocs-ci.sh test-ocs-ci.sh [ --tier <0,1,...> ] teardown-ocs-ci.sh destroy-ocp.sh This project uses the following git submodules: github.com/ocp-power-automation/ocp4-upi-kvm github.com/red-hat-storage/ocs-ci These underlying projects must be instantiated before the create, setup, deploy, test, and teardown scripts are used. The user is expected to setup the submodules before invoking these scripts. The workflow sample scripts described in the next section provide some end to end work flows which of necessity instantiate submodules. These sample scripts may be copied to the workspace directory and edited as desired to customize a work flow. Most users are expected to do this. The information provided below describes some of the dynamics surrounding the create, deploy, and test scripts. First, there are two ways to instantiate submodules as shown below: git clone https://github.com/ocp-power-automation/ocs-upi-kvm --recursive cd ocs-upi-kvm/scripts OR git clone https://github.com/ocp-power-automation/ocs-upi-kvm.sh cd ocs-upi-kvm git submodule update --init The majority of the create-ocp.sh command is spent running terraform (and ansible). On occasion, a transient error will occur while creating the cluster. In this case, the operation can be restarted by specifying the --retry argument. This can save half an hour of execution time. If this argument is not specified, the existing cluster will be torn down automatically assuming there is one. If a failure occurs while running the deploy-ocs-ci.sh script, the operation has to be restarted from the beginning. That is to say with creat-ocp.sh . Do not specify the --retry argument as the OCP cluster has to be completely removed before trying to deploy OCS. The ocs-ci project alters the state of the OCP cluster. Further, the teardown-ocs-ci.sh script has never been obcserved to work cleanly. This simply invokes the underlying ocs-ci function. It is provided as it may be fixed in time and it is a valuable function, if only in theory now. The script destroy-ocp.sh which recompletely removes ocp and ocs. The script create-ocp.sh will also remove an existing OCP cluster if one is present before creating a new one as only one OCP cluster is supported on the host KVM server at a time . This is true even if the cluster was created by another user, so if you are concerned with impacting other users run this command first, sudo virsh list --all .","title":"Scripts"},{"location":"ocs_kvm/#workflow-sample-scripts","text":"samples/dev-ocs.sh [--retry-ocp] [--latest-ocs] samples/test-ocs.sh [--retry-ocp] [--latest-ocs] These scripts are useful in getting started. They implement the full sequence of high level tasks defined above. The test-ocs.sh invokes ocs-ci tier tests 2, 3, 4, 4a, 4b, and 4c. Both scripts designate the use of file backed Ceph disks which are based on qcow2 files. These files are sparsely populated enabling the use of servers with as little as 256 GBs of storage, depending on the number of worker nodes that are requested. The use of file backed data disks is the default. The test-ocs scripts include comments showing how physical disk partitions may be used instead which may improve performance and resilience. As noted above, these scripts may be relocated, customized, and invoked from the workspace directory.","title":"Workflow Sample Scripts"},{"location":"ocs_kvm/#required-environment-variables","text":"RHID_USERNAME=xxx RHID_PASSWORD=yyy OR RHID_ORG=ppp RHID_KEY=qqq","title":"Required Environment Variables"},{"location":"ocs_kvm/#project-workspace","text":"This project is designed to be used in an automated test framework like Jenkins which utilizes dedicated workspaces to run jobs in parallel on the same server. As such, all input, output, and internally generated files are restricted to the specific workspace instance that is allocated to run the job. For this project, that workspace is assumed to be the parent directory of the cloned project ocs-upi-kvm itself.","title":"Project Workspace"},{"location":"ocs_kvm/#required-files","text":"These files should be placed in the workspace directory. ~/auth.yaml ~/pull-secret.txt ~/$BASTION_IMAGE The auth.yaml file is required for the script deploy-ocs-ci.sh. It contains secrets for quay and quay.io/rhceph-dev which are obtained from the Redhat OCS-CI team. The pull-secret.txt is required for the scripts create-ocp.sh and deploy-ocs-ci.sh. Download your managed pull secrets from https://cloud.redhat.com/openshift/install/pull-secret and add the secret for quay.io/rhceph-dev noted above to the pull-secret.txt file. You will also need to add the secret for registry.svc.ci.openshift.org which may be obtained as follows: Become a member of openshift organization login to https://api.ci.openshift.org/console/catalog Click on \"copy login command\" under username in the right corner. (This will copy the oc login command to your clipboard.) Now open your terminal, paste from the clipboard buffer and execute that command you just pasted (oc login). Execute the oc registry login --registry registry.svc.ci.openshift.org which will store your token in ~/.docker/config.json. The bastion image is a prepared image downloaded from the Red Hat Customer Portal following these instructions . It is named by the environment variable BASTION_IMAGE which has a default value of \"rhel-8.2-update-2-ppc64le-kvm.qcow2\". This is the name of the file downloaded from the RedHat website. When preparing the bastion image above, the root password must be set to 123456 .","title":"Required Files"},{"location":"ocs_kvm/#optional-environment-variables-with-default-values","text":"OCP_VERSION=${OCP_VERSION:=\"4.5\"} CLUSTER_DOMAIN=${CLUSTER_DOMAIN:=\"tt.testing\"} BASTION_IMAGE=${BASTION_IMAGE:=\"rhel-8.2-update-2-ppc64le-kvm.qcow2\"} MASTER_DESIRED_CPU=${MASTER_DESIRED_CPU:=\"4\"} MASTER_DESIRED_MEM=${MASTER_DESIRED_MEM:=\"16384\"} WORKER_DESIRED_CPU=${WORKER_DESIRED_CPU:=\"16\"} WORKER_DESIRED_MEM=${WORKER_DESIRED_MEM:=\"65536\"} WORKERS=${WORKERS:=3} IMAGES_PATH=${IMAGES_PATH:=\"/var/lib/libvirt/images\"} OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=${OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE:=\"\"} DATA_DISK_SIZE=${DATA_DISK_SIZE:=256} DATA_DISK_LIST=${DATA_DISK_LIST:=\"\"} FORCE_DISK_PARTITION_WIPE=${FORCE_DISK_PARTITION_WIPE:=\"false\"} CHRONY_CONFIG=${CHRONY_CONFIG:=\"true\"} RHCOS_RELEASE=${RHCOS_RELEASE:=\"\"} Disk sizes are in GBs. The CHRONY_CONFIG parameter above enables NTP servers as OCS CI expects them to be configured. If that is not applicable, then this parameter should probably be set to false. The RHCOS_RELEASE parameter is specific to the OCP_VERSION and is set internally to the latest available rhcos image available provided that it is not set by the user. The internal setting may be outdated. The supported OCP_VERSION s are 4.4 - 4.7. Set a new value like this: export OCP_VERSION=4.6 export RHCOS_RELEASE=4.6.1 The OpenShift installer uses the latest available image which by default is a development build. For released OCP versions, this tool will chose a recently released image based on the environment OCP_VERSION. This image may not be latest available version. This internal selection may be overriden by setting the environment variable OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE to a development preview image . Set a specific daily build like this: REGISTRY=\"registry.svc.ci.openshift.org/ocp-ppc64le/release-ppc64le\" export OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=\"$REGISTRY:4.6.0-0.nightly-ppc64le-2020-09-27-075246\" The script create-ocp.sh will add a data disk to each worker node. This disk is presented inside the worker node as /dev/vdc. In the KVM host server OS, the data disk is backed by either a file or a physical disk partition. If you specify the environment variable DATA_DISK_LIST, then the named physical disk partitions (/dev) will be used. The list is composed of comma separated unique partition names with one partition name specified per worker node. For example, export DATA_DISK_LIST=\"sdi1,sdi2,sdi3\" Otherwise, the data disks will be backed by a file. The environment variable DATA_DISK_SIZE controls the size of the file allocation. If you don't want the extra disk to be allocated, then set DATA_DISK_SIZE=0. In this case, don't run the scripts setup-ocs-ci.sh or deploy-ocs-ci.sh as they will fail. The environment variable FORCE_DISK_PARTITION_WIPE may be set to 'true' to wipe the data on a hard disk partition assuming the environment variable DATA_DISK_LIST is specified. The wipe may take an hour or more to complete.","title":"Optional Environment Variables with Default Values"},{"location":"ocs_kvm/#post-creation-of-the-openshift-cluster","text":"Build artifacts are placed in the workspace directory which is defined as the parent directory of this github project ocs-upi-kvm . The examples shown below use a dedicated directory for this purpose as there are quite a few output files, some of which are not shown below such as rpms and tar files that are downloaded during cluster creation.","title":"Post Creation of The OpenShift Cluster"},{"location":"ocs_kvm/#the-oc-command","text":"Upon successful completion of the create-ocp.sh script, the oc command may be invoked in the following way: [user@kvm-host workspace]; source env-ocp.sh [user@kvm-host workspace]; oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 40h v1.19.0+d59ce34 master-1 Ready master 40h v1.19.0+d59ce34 master-2 Ready master 40h v1.19.0+d59ce34 worker-0 Ready worker 39h v1.19.0+d59ce34 worker-1 Ready worker 39h v1.19.0+d59ce34 worker-2 Ready worker 39h v1.19.0+d59ce34 The env-ocp.sh script exports KUBECONFIG and updates the PATH environment variable. It may be useful in some cases to stick these in your user profile.","title":"The oc Command"},{"location":"ocs_kvm/#log-files","text":"The following log files are produced: [user@kvm-host-ahv workspace]$ ls -lt *log -rw-rw-r--. 1 luke luke 409491 Oct 23 18:36 create-ocp.log -rw-rw-r--. 1 luke luke 654998 Oct 23 19:06 deploy-ocs-ci.log -rw-rw-r--. 1 luke luke 1144731 Oct 22 23:23 ocs-ci.git.log -rw-rw-r--. 1 luke luke 18468 Oct 23 18:38 setup-ocs-ci.log -rw-rw-r--. 1 luke luke 23431620 Oct 23 18:35 terraform.log -rw-rw-r--. 1 luke luke 29235845 Oct 25 00:30 test-ocs-ci.log","title":"Log Files"},{"location":"ocs_kvm/#remote-webconsole-support","text":"The cluster create command outputs webconsole information which should look something like the first entry below. This information needs to be added to your /etc/hosts file or MacOS equivalent defining the IP Address of the KVM host server. You must generate the companion oauth definition as shown below following the same pattern. <ip kvm host server> console-openshift-console.apps.test-ocp4-5.tt.testing oauth-openshift.apps.test-ocp4-5.tt.testing The browser should prompt you to login to the OCP cluster. The user name is kubeadmin and the password is located in the file /auth/kubeadmin-password .","title":"Remote Webconsole Support"},{"location":"ocs_kvm/#chrontab-automation","text":"The following two files have been provided: chron-ocs.sh test-chron-ocs.sh The chron-ocs.sh script is the master chrontab commandline script. It is located in the scripts/helper directory. The test-chron-ocs.sh script is invoked by chron-ocs.sh and provides the end-to-end OCP/OCS command flow. Presently, this script invokes tier tests 2, 3, 4, 4a, 4b and 4c. You can limit the tests to a subset by editing this file. This file is located in the samples directory. To setup chrontab automation, you must: Create test user account with sudo authority and login to it git clone this project in $HOME and invoke scripts/helper/set-passwordless-sudo.sh Place the required files defined by the ocs-upi-kvm project in $HOME Copy the two chron scripts listed above to $HOME Edit the four lines below in test-chron-ocs.sh : export RHID_USERNAME=<Your RedHat Subscription id> export RHID_PASSWORD=<RedHat Subscription password> export OCP_VERSION=4.5 export IMAGES_PATH=/home/libvirt/images Invoke crontab -e and enter the following two lines: SHELL=/bin/bash 0 0 * * * ~/chron-ocs.sh The example above will invoke chron-ocs.sh every 24 hours at midnight local time. Log files are written to the logs directory under the user's home directory.","title":"Chrontab Automation"},{"location":"playbook/","text":"Introduction The playbooks are used for installation of OCP on Power and other post install customizations. The playbooks are used with PowerVS , PowerVC and KVM projects. Assumptions A bastion/helper node is already created where the playbooks would run. The required services are configured on the bastion/helper node using helpernode playbook . The cluster nodes are already created. Bastion HA setup We can have multiple bastion nodes as part of the OpenShift 4.X setup. Ensure that all the required services are configured on all the bastion nodes. Also, keepalived service should be configured and running. To use this playbook for bastion HA you need to: 1. Run helpernode playbook with HA configurations . 1. Use bastion_vip variable with keepalived vrrp address. 1. Add bastion host group entries with all bastion nodes in examples/inventory . The OpenShift install commands will always run on the first bastion. Additional services such as squid proxy, chrony, etc. will be configured on all nodes. The auth directory and ignition files will be available on all nodes once the installation complete. Set up the required variables Make use of the sample file at examples/install_vars.yaml . Modify the values as per your cluster. cp examples/install_vars.yaml . Use install_vars.yaml This section sets the variables for the install-config.yaml template file. install_config: cluster_domain: < Cluster domain name. Match to the baseDomain in install-config.yaml.> cluster_id: < Cluster identifier. Match to the metadata.name in install-config.yaml.> pull_secret: '<pull-secret json content>' public_ssh_key: '<SSH public key content to access the cluster nodes>' Below variables will be used by the OCP install playbook. workdir: <Directory to use for creating OCP configs> storage_type: <Storage type used in the cluster. Eg: nfs (Note: Currently NFS provisioner is not configured using this playbook. This variable is only used for setting up image registry to EmptyDir if storage_type is not nfs)> log_level: <Option --log-level in openshift-install commands. Default is 'info'> release_image_override: '<This is set to OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE while creating ign files. If you are using internal artifactory then ensure that you have added auth key to the pull_secret>' rhcos_kernel_options: <List of kernel options for RHCOS nodes eg: [\"slub_max_order=0\",\"loglevel=7\"]> Setting up inventory Make use of sample file at examples/inventory . Modify the host values as per your cluster. cp examples/inventory . Run the playbook Once you have configured the vars & inventory file, run the install playbook using: ansible-playbook -i inventory -e @install_vars.yaml playbooks/install.yaml License See LICENCE.txt","title":"OCP4-Playbooks"},{"location":"playbook/#introduction","text":"The playbooks are used for installation of OCP on Power and other post install customizations. The playbooks are used with PowerVS , PowerVC and KVM projects.","title":"Introduction"},{"location":"playbook/#assumptions","text":"A bastion/helper node is already created where the playbooks would run. The required services are configured on the bastion/helper node using helpernode playbook . The cluster nodes are already created.","title":"Assumptions"},{"location":"playbook/#bastion-ha-setup","text":"We can have multiple bastion nodes as part of the OpenShift 4.X setup. Ensure that all the required services are configured on all the bastion nodes. Also, keepalived service should be configured and running. To use this playbook for bastion HA you need to: 1. Run helpernode playbook with HA configurations . 1. Use bastion_vip variable with keepalived vrrp address. 1. Add bastion host group entries with all bastion nodes in examples/inventory . The OpenShift install commands will always run on the first bastion. Additional services such as squid proxy, chrony, etc. will be configured on all nodes. The auth directory and ignition files will be available on all nodes once the installation complete.","title":"Bastion HA setup"},{"location":"playbook/#set-up-the-required-variables","text":"Make use of the sample file at examples/install_vars.yaml . Modify the values as per your cluster. cp examples/install_vars.yaml .","title":"Set up the required variables"},{"location":"playbook/#use-install_varsyaml","text":"This section sets the variables for the install-config.yaml template file. install_config: cluster_domain: < Cluster domain name. Match to the baseDomain in install-config.yaml.> cluster_id: < Cluster identifier. Match to the metadata.name in install-config.yaml.> pull_secret: '<pull-secret json content>' public_ssh_key: '<SSH public key content to access the cluster nodes>' Below variables will be used by the OCP install playbook. workdir: <Directory to use for creating OCP configs> storage_type: <Storage type used in the cluster. Eg: nfs (Note: Currently NFS provisioner is not configured using this playbook. This variable is only used for setting up image registry to EmptyDir if storage_type is not nfs)> log_level: <Option --log-level in openshift-install commands. Default is 'info'> release_image_override: '<This is set to OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE while creating ign files. If you are using internal artifactory then ensure that you have added auth key to the pull_secret>' rhcos_kernel_options: <List of kernel options for RHCOS nodes eg: [\"slub_max_order=0\",\"loglevel=7\"]>","title":"Use install_vars.yaml"},{"location":"playbook/#setting-up-inventory","text":"Make use of sample file at examples/inventory . Modify the host values as per your cluster. cp examples/inventory .","title":"Setting up inventory"},{"location":"playbook/#run-the-playbook","text":"Once you have configured the vars & inventory file, run the install playbook using: ansible-playbook -i inventory -e @install_vars.yaml playbooks/install.yaml","title":"Run the playbook"},{"location":"playbook/#license","text":"See LICENCE.txt","title":"License"},{"location":"powervm/","text":"Table of Contents Introduction Pre-requisites Image and LPAR requirements OCP Install Contributing Introduction This repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on PowerVM LPARs. This assumes PowerVC is used as the IaaS layer for managing the PowerVM LPARs. If you are using standalone PowerVM please take a look at the following quickstart guide which uses the ansible playbook to setup helper node (bastion) for OCP deployment. This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC. Run this code from either Mac or Linux (Intel) system. For automating setup, see OCP-UPI-PowerVM :heavy_exclamation_mark: This automation is intended for test/development purposes only and there is no formal support. For bugs/enhancement requests etc. please open a GitHub issue Pre-requisites You need to identify a remote client machine for running the automation. This could be your laptop or a VM. Operating Systems This code has been tested on the following x86-64 based Operating Systems: - Linux - Mac OSX (Darwin) Packages Install the below required packages on the client machine. Git : Please refer to the link for instructions on installing the latest Git. Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Public internet connection for all the nodes to download the playbooks and images as part of the setup process. Image and LPAR requirements You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.0 (or later) image in PowerVC. For RHCOS image creation, follow the steps mentioned in the following doc . Following are the recommended LPAR configs for OpenShift nodes that will be deployed with RHCOS image. - Bootstrap, Master - 2 vCPUs, 16GB RAM, 120 GB Disk. PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be 16 ( 2 vCPUs x 8 SMT ) This config is suitable for majority of the scenarios - Worker - 2 vCPUs, 16GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Following is the recommended LPAR config for the helper node that will be deployed with RHEL 8.0 (or later) image. - Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk OCP Install Follow these quickstart steps to kickstart OCP installation on PowerVM LPARs managed via PowerVC Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"OCP4-UPI-PowerVM"},{"location":"powervm/#table-of-contents","text":"Introduction Pre-requisites Image and LPAR requirements OCP Install Contributing","title":"Table of Contents"},{"location":"powervm/#introduction","text":"This repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on PowerVM LPARs. This assumes PowerVC is used as the IaaS layer for managing the PowerVM LPARs. If you are using standalone PowerVM please take a look at the following quickstart guide which uses the ansible playbook to setup helper node (bastion) for OCP deployment. This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC. Run this code from either Mac or Linux (Intel) system. For automating setup, see OCP-UPI-PowerVM :heavy_exclamation_mark: This automation is intended for test/development purposes only and there is no formal support. For bugs/enhancement requests etc. please open a GitHub issue","title":"Introduction"},{"location":"powervm/#pre-requisites","text":"You need to identify a remote client machine for running the automation. This could be your laptop or a VM.","title":"Pre-requisites"},{"location":"powervm/#operating-systems","text":"This code has been tested on the following x86-64 based Operating Systems: - Linux - Mac OSX (Darwin)","title":"Operating Systems"},{"location":"powervm/#packages","text":"Install the below required packages on the client machine. Git : Please refer to the link for instructions on installing the latest Git. Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Public internet connection for all the nodes to download the playbooks and images as part of the setup process.","title":"Packages"},{"location":"powervm/#image-and-lpar-requirements","text":"You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.0 (or later) image in PowerVC. For RHCOS image creation, follow the steps mentioned in the following doc . Following are the recommended LPAR configs for OpenShift nodes that will be deployed with RHCOS image. - Bootstrap, Master - 2 vCPUs, 16GB RAM, 120 GB Disk. PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be 16 ( 2 vCPUs x 8 SMT ) This config is suitable for majority of the scenarios - Worker - 2 vCPUs, 16GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Following is the recommended LPAR config for the helper node that will be deployed with RHEL 8.0 (or later) image. - Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk","title":"Image and LPAR requirements"},{"location":"powervm/#ocp-install","text":"Follow these quickstart steps to kickstart OCP installation on PowerVM LPARs managed via PowerVC","title":"OCP Install"},{"location":"powervm/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"powervs/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install Contributing Introduction PowerVS contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . This project leverages the helpernode ansible playbook internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS). :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue For general PowerVS usage instructions please refer to the following links: - https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-getting-started - https://www.youtube.com/watch?v=RywSfXT_LLs - https://www.youtube.com/playlist?list=PLVrJaTKVPbKM_9HU8fm4QsklgzLGUwFpv :information_source: This branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - {release-4.5, release-4.6 ...} and follow the docs. Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems: - Mac OSX (Darwin) - Linux (x86_64) - Windows 10 Follow the guide to complete the prerequisites. PowerVS Prerequisites Follow the guide to complete the PowerVS prerequisites. OCP Install Follow the quickstart guide for OCP installation on PowerVS. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"OCP4-UPI-PowerVS"},{"location":"powervs/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"powervs/#introduction","text":"PowerVS contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . This project leverages the helpernode ansible playbook internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS). :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue For general PowerVS usage instructions please refer to the following links: - https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-getting-started - https://www.youtube.com/watch?v=RywSfXT_LLs - https://www.youtube.com/playlist?list=PLVrJaTKVPbKM_9HU8fm4QsklgzLGUwFpv :information_source: This branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - {release-4.5, release-4.6 ...} and follow the docs.","title":"Introduction"},{"location":"powervs/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems: - Mac OSX (Darwin) - Linux (x86_64) - Windows 10 Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"powervs/#powervs-prerequisites","text":"Follow the guide to complete the PowerVS prerequisites.","title":"PowerVS Prerequisites"},{"location":"powervs/#ocp-install","text":"Follow the quickstart guide for OCP installation on PowerVS.","title":"OCP Install"},{"location":"powervs/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"terraform/","text":"Terraform Provider Website: https://www.terraform.io Mailing list: Google Groups Requirements Terraform 0.10.1+ Go 1.13 (to build the provider plugin) Building The Provider Clone repository to: $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm mkdir -p $GOPATH/src/github.com/IBM-Cloud; cd $GOPATH/src/github.com/IBM-Cloud git clone git@github.com:IBM-Cloud/terraform-provider-ibm.git Enter the provider directory and build the provider cd $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm make build Docker Image For The Provider You can also pull the docker image for the ibmcloud terraform provider : docker pull ibmterraform/terraform-provider-ibm-docker Using the provider If you want to run Terraform with the IBM Cloud provider plugin on your system, complete the following steps: Download and install Terraform for your system . Download the IBM Cloud provider plugin for Terraform . Unzip the release archive to extract the plugin binary ( terraform-provider-ibm_vX.Y.Z ). Move the binary into the Terraform plugins directory for the platform. Linux/Unix/OS X: ~/.terraform.d/plugins Windows: %APPDATA%\\terraform.d\\plugins Export API credential tokens as environment variables. This can either be IBM Cloud API keys or Softlayer API keys and usernames, depending on the resources you are provisioning. export IC_API_KEY=\"IBM Cloud API Key\" export IAAS_CLASSIC_API_KEY=\"IBM Cloud Classic Infrastructure API Key\" export IAAS_CLASSIC_USERNAME=\"IBM Cloud Classic Infrastructure username associated with Classic Infrastructure API KEY\". Add the plug-in provider to the Terraform configuration file. provider \"ibm\" {} See the official documentation for more details on using the IBM provider. Developing the Provider If you wish to work on the provider, you'll first need Go installed on your machine (version 1.8+ is required ). You'll also need to correctly setup a GOPATH , as well as adding $GOPATH/bin to your $PATH . To compile the provider, run make build . This will build the provider and put the provider binary in the $GOPATH/bin directory. make build ... $GOPATH/bin/terraform-provider-ibm ... In order to test the provider, you can simply run make test . make test In order to run the full suite of Acceptance tests, run make testacc . Note: Acceptance tests create real resources, and often cost money to run. make testacc In order to run a particular Acceptance test, export the variable TESTARGS . For example export TESTARGS=\"-run TestAccIBMNetworkVlan_Basic\" Issuing make testacc will now run the testcase with names matching TestAccIBMNetworkVlan_Basic . This particular testcase is present in ibm/resource_ibm_network_vlan_test.go You will also need to export the following environment variables for running the Acceptance tests. * IC_API_KEY - The IBM Cloud API Key * IAAS_CLASSIC_API_KEY - The IBM Cloud Classic Infrastructure API Key * IAAS_CLASSIC_USERNAME - The IBM Cloud Classic Infrastructure username associated with the Classic InfrastAPI Key. Additional environment variables may be required depending on the tests being run. Check console log for warning messages about required variables. IBM Cloud Ansible Modules An implementation of generated Ansible modules using the IBM Cloud Terraform Provider . Prerequisites Install Python3 RedHat Ansible 2.8+ pip install \"ansible>=2.8.0\" Install Download IBM Cloud Ansible modules from release page Extract module archive. unzip ibmcloud_ansible_modules.zip Add modules and module_utils to the Ansible search path . E.g.: ``` cp build/modules/ $HOME/.ansible/plugins/modules/. cp build/module_utils/ $HOME/.ansible/plugins/module_utils/. ``` Example Projects VPC Virtual Server Instance Power Virtual Server Instance","title":"Terraform-Provider-IBM"},{"location":"terraform/#terraform-provider","text":"Website: https://www.terraform.io Mailing list: Google Groups","title":"Terraform Provider"},{"location":"terraform/#requirements","text":"Terraform 0.10.1+ Go 1.13 (to build the provider plugin)","title":"Requirements"},{"location":"terraform/#building-the-provider","text":"Clone repository to: $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm mkdir -p $GOPATH/src/github.com/IBM-Cloud; cd $GOPATH/src/github.com/IBM-Cloud git clone git@github.com:IBM-Cloud/terraform-provider-ibm.git Enter the provider directory and build the provider cd $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm make build","title":"Building The Provider"},{"location":"terraform/#docker-image-for-the-provider","text":"You can also pull the docker image for the ibmcloud terraform provider : docker pull ibmterraform/terraform-provider-ibm-docker","title":"Docker Image For The Provider"},{"location":"terraform/#using-the-provider","text":"If you want to run Terraform with the IBM Cloud provider plugin on your system, complete the following steps: Download and install Terraform for your system . Download the IBM Cloud provider plugin for Terraform . Unzip the release archive to extract the plugin binary ( terraform-provider-ibm_vX.Y.Z ). Move the binary into the Terraform plugins directory for the platform. Linux/Unix/OS X: ~/.terraform.d/plugins Windows: %APPDATA%\\terraform.d\\plugins Export API credential tokens as environment variables. This can either be IBM Cloud API keys or Softlayer API keys and usernames, depending on the resources you are provisioning. export IC_API_KEY=\"IBM Cloud API Key\" export IAAS_CLASSIC_API_KEY=\"IBM Cloud Classic Infrastructure API Key\" export IAAS_CLASSIC_USERNAME=\"IBM Cloud Classic Infrastructure username associated with Classic Infrastructure API KEY\". Add the plug-in provider to the Terraform configuration file. provider \"ibm\" {} See the official documentation for more details on using the IBM provider.","title":"Using the provider"},{"location":"terraform/#developing-the-provider","text":"If you wish to work on the provider, you'll first need Go installed on your machine (version 1.8+ is required ). You'll also need to correctly setup a GOPATH , as well as adding $GOPATH/bin to your $PATH . To compile the provider, run make build . This will build the provider and put the provider binary in the $GOPATH/bin directory. make build ... $GOPATH/bin/terraform-provider-ibm ... In order to test the provider, you can simply run make test . make test In order to run the full suite of Acceptance tests, run make testacc . Note: Acceptance tests create real resources, and often cost money to run. make testacc In order to run a particular Acceptance test, export the variable TESTARGS . For example export TESTARGS=\"-run TestAccIBMNetworkVlan_Basic\" Issuing make testacc will now run the testcase with names matching TestAccIBMNetworkVlan_Basic . This particular testcase is present in ibm/resource_ibm_network_vlan_test.go You will also need to export the following environment variables for running the Acceptance tests. * IC_API_KEY - The IBM Cloud API Key * IAAS_CLASSIC_API_KEY - The IBM Cloud Classic Infrastructure API Key * IAAS_CLASSIC_USERNAME - The IBM Cloud Classic Infrastructure username associated with the Classic InfrastAPI Key. Additional environment variables may be required depending on the tests being run. Check console log for warning messages about required variables.","title":"Developing the Provider"},{"location":"terraform/#ibm-cloud-ansible-modules","text":"An implementation of generated Ansible modules using the IBM Cloud Terraform Provider .","title":"IBM Cloud Ansible Modules"},{"location":"terraform/#prerequisites","text":"Install Python3 RedHat Ansible 2.8+ pip install \"ansible>=2.8.0\"","title":"Prerequisites"},{"location":"terraform/#install","text":"Download IBM Cloud Ansible modules from release page Extract module archive. unzip ibmcloud_ansible_modules.zip Add modules and module_utils to the Ansible search path . E.g.: ``` cp build/modules/ $HOME/.ansible/plugins/modules/. cp build/module_utils/ $HOME/.ansible/plugins/module_utils/. ```","title":"Install"},{"location":"terraform/#example-projects","text":"VPC Virtual Server Instance Power Virtual Server Instance","title":"Example Projects"}]}